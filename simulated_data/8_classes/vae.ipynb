{"cells":[{"cell_type":"code","execution_count":null,"id":"96c9ec14","metadata":{"id":"96c9ec14"},"outputs":[],"source":["path_to_module = '/content/drive/MyDrive/TESS-ZTF-Transient-Classification/'\n","\n","import sys\n","import pandas as pd\n","import keras\n","import pickle\n","import numpy as np\n","from os import walk\n","import os\n","import matplotlib.pyplot as plt\n","import pickle as pkl\n","import math\n","import random\n","import keras.backend as K\n","import tensorflow as tf\n","\n","from sys import path\n","from google.colab import drive, files\n","from collections import Counter\n","from warnings import filterwarnings\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from keras import layers, Input, Model\n","from keras.callbacks import EarlyStopping\n","from keras.layers import (GRU, Dense, Lambda, Masking, RepeatVector, TimeDistributed, concatenate)\n","from keras.optimizers import adam_v2\n","from sklearn.manifold import TSNE\n","from tensorflow.python.framework.ops import disable_eager_execution\n","\n","disable_eager_execution()\n","filterwarnings(\"ignore\")\n","path.append(path_to_module)\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["# define constants\n","R_BAND = 6.5\n","G_BAND = 4.8\n","TESS_BAND = 7.9\n","EPOCHS_PER_CHECKPOINT = 20"],"metadata":{"id":"OzrRCnVZPDLc"},"id":"OzrRCnVZPDLc","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4ed89428","metadata":{"id":"4ed89428"},"outputs":[],"source":["data = None\n","with open(path_to_module + 'light_curves_sims/separated_classes/numpy_files/dataset.pickle', 'rb') as file:\n","  data = pickle.load(file)\n","# data is list of numpy arrays:\n","# first array is light curve names\n","# second one is light curve labels\n","# third one is light curves\n","names, labels, dataset = data\n","labels = np.array(labels)\n","\n","# normalize data using min-max range\n","for i, xarr in enumerate(dataset):\n","    mask = np.where(xarr[:, 2:3] != 0)[0]\n","    maxval = xarr[:, 2:3][mask].max()\n","    minval = xarr[:, 2:3][mask].min()\n","    dataset[i, :, 2:3][mask] = dataset[i, :, 2:3][mask] / (maxval-minval) \n","    dataset[i, :, 3:4][mask] = dataset[i, :, 3:4][mask] / (maxval-minval)"]},{"cell_type":"code","source":["light_curve = dataset[0]\n","r_time, r_flux = [time for time, band, flux, error in light_curve if band == 6.5]\n","print(r_time)"],"metadata":{"id":"91IFJIM6KqjQ"},"id":"91IFJIM6KqjQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for light_curve in dataset[0:16000:800]:\n","  r_time = []\n","  r_flux = []\n","  g_time = []\n","  g_flux = []\n","  # print(light_curve)\n","  for time, band, flux, error in light_curve:\n","    # green band\n","    if band == 4.8:\n","      g_time.append(time)\n","      g_flux.append(flux)\n","    else:\n","      r_time.append(time)\n","      r_flux.append(flux)\n","  # print(r_flux)\n","  fig, ax1 = plt.subplots(1)\n","  ax1.scatter(g_time, g_flux, color = 'green')\n","  ax1.scatter(r_time, r_flux, color = 'red')\n","  fig.show()"],"metadata":{"id":"rDaTjWaeSJO1"},"id":"rDaTjWaeSJO1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1d5ee600","metadata":{"id":"1d5ee600"},"outputs":[],"source":["class Sampling(layers.Layer):\n","  def call(self, inputs):\n","    z_mean, z_log_var = inputs\n","    batch = K.shape(z_mean)[0]\n","    dim = K.shape(z_mean)[1]\n","    epsilon = K.random_normal(shape=(batch, dim))\n","    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n","class Encoder(keras.Model):\n","  def __init__(self, shapes, mask_val=0, name = 'encoder', **kwargs):\n","    super(Encoder, self).__init__(name=name, **kwargs)\n","    self.mask_val = mask_val\n","    self.shapes = shapes\n","\n","    # DEFINE ENCODER LAYERS\n","    self.enc_input = Input(shape=shapes['enc_input'])\n","    \n","    # get mask for future layer\n","    self.mask = Masking(mask_value=mask_val)\n","\n","    # first recurrent layer\n","    self.gru1 = GRU(shapes['gru1'], activation='tanh', \n","                    recurrent_activation='hard_sigmoid', return_sequences=True, name='gru1')\n","    # second recurrent layer  \n","    self.encoded = GRU(shapes['gru2'], activation='tanh',\n","                    recurrent_activation='hard_sigmoid', return_sequences=True, name='gru2')\n","    \n","    # z mean output\n","    self.z_mean = GRU(shapes['gru3'], return_sequences=False, activation='linear', name='gru3')\n","\n","    # z variance output\n","    self.z_log_var = GRU(shapes['gru4'], return_sequences=False, activation='linear', name='gru4')\n","\n","    # sample output\n","    self.z = Sampling()\n","\n","  def get_config(self):\n","      # config = super(Encoder, self).get_config()\n","      config = {\"shapes\": self.shapes, 'mask_val':0, 'name':'encoder'}\n","      return config\n","\n","  # define forward pass\n","  def call(self, inputs):\n","    \n","    mask_compute = self.mask(inputs)\n","    gru1 = self.gru1(mask_compute)\n","    mask_output = self.mask.compute_mask(gru1)\n","\n","    encoded = self.encoded(gru1)\n","    z_mean = self.z_mean(encoded)\n","    z_log_var = self.z_log_var(encoded)\n","    z = self.z([z_mean, z_log_var])\n","\n","    return z_mean, z_log_var, z\n","\n","class Decoder(keras.Model):\n","  def __init__(self, shapes, mask_val=0, name = 'decoder', **kwargs):\n","    super(Decoder, self).__init__(name=name, **kwargs)\n","    self.mask_val = mask_val\n","    self.shapes = shapes\n","\n","    # define layers\n","    self.decoder_input = Input(shape=shapes['dec_input'])\n","\n","    self.repeater = RepeatVector(shapes['repeater'], name='rep')\n","\n","    # time and filter ID vals\n","    self.input_two = Input(shape=shapes['input_two'])\n","\n","    # first recurrent layer\n","    self.gru5 = GRU(shapes['gru5'], activation='tanh',\n","                    recurrent_activation='hard_sigmoid', return_sequences=True, name='gru5')\n","    \n","    # second recurrent layer\n","    self.gru6 = GRU(shapes['gru6'], activation='tanh',\n","                    recurrent_activation='hard_sigmoid', return_sequences=True, name='gru6')\n","\n","    # decoder output\n","    self.dec_output = TimeDistributed(\n","        Dense(1, activation='tanh', input_shape=shapes['dec_output']), name='td')\n","\n","  def get_config(self):\n","      config = {\"shapes\": self.shapes, 'mask_val':0, 'name':'decoder'}\n","      return config\n","\n","  # define forward pass\n","  def call(self, inputs):\n","    z, train_input_two = inputs\n","    repeater = self.repeater(z)\n","    concat = concatenate((repeater, train_input_two), axis =-1)\n","    gru5 = self.gru5(concat)\n","    gru6 = self.gru6(gru5)\n","    dec_output = self.dec_output(gru6)\n","\n","    return dec_output\n","\n","class VAE(keras.Model):\n","  def __init__(self, prepared_data, dims=(16000,80,4), name='vae', **kwargs):\n","    super(VAE, self).__init__(name=name, **kwargs)\n","    self.epochs = 20\n","    self.batch_size = 64\n","    self.optimizer = 'adam'\n","\n","    # dimension of the latent vector\n","    self.latent_dim = 30\n","\n","    # input to first encoder and second decoder layer\n","    self.gru_one = 175\n","\n","    # input to first decoder and second encoder layer\n","    self.gru_two = 150\n","\n","    # load prepared dad (acts a input)\n","    self.prepared_data = np.array(prepared_data)\n","    \n","    # number of input features\n","    self.num_feats = dims[2]\n","    \n","    # number of timesteps\n","    self.num_timesteps = dims[1]\n","\n","    # dimension of the input space for encoder\n","    self.enc_input_shape = (self.num_timesteps, self.num_feats)\n","\n","    # number of light curves\n","    self.num_lcs = dims[0]\n","\n","    # layer dimensions for encoder and decoder, respectively\n","    self.enc_dims = {\n","      'enc_input': self.enc_input_shape,\n","      'gru1': self.gru_one,\n","      'gru2': self.gru_two,\n","      'gru3': self.latent_dim,\n","      'gru4': self.latent_dim\n","    }\n","    self.dec_dims = {\n","      'dec_input': self.latent_dim,\n","      'repeater': self.num_timesteps,\n","      'input_two': (self.num_timesteps, 2),\n","      'gru5': self.gru_two,\n","      'gru6': self.gru_one,\n","      'dec_output': (None, 1)\n","    }\n","\n","\n","    # indxs for test and train\n","    self.train_indx = set()\n","    self.test_indx  = set()\n","\n","    self.mask_value = 0.0\n","    \n","    self.encoder = Encoder(self.enc_dims)\n","    self.decoder = Decoder(self.dec_dims)\n","\n","  def get_config(self):\n","      config = {\"prepared_data\": np.array(self.prepared_data), 'name':'vae'}\n","      return config\n","\n","  # define forward pass\n","  def call(self, inputs):\n","    x_train, train_input_two = inputs\n","    z_mean, z_log_var, z = self.encoder(x_train)\n","    reconstructed = self.decoder([z, train_input_two])\n","\n","    # Add KL divergence regularization loss.\n","    kl_loss = -0.5 * tf.reduce_mean(\n","        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n","    )\n","    self.add_loss(kl_loss)\n","    return reconstructed\n","\n","  def reconstruction_loss(self, yTrue, yPred):\n","    return K.log(K.mean(K.square(yTrue - yPred)))\n","\n","  def split_prep_data(self):\n","      \"\"\"\n","      Splits data into 3/4 training, 1/4 testing\n","      \"\"\"\n","\n","      print(\"Splitting data into train and test...\")\n","\n","      # prepared out (only flux)\n","      prep_out = self.prepared_data[:, :, 2].reshape(\n","          self.num_lcs, self.num_timesteps, 1)\n","      # prep_out = self.prepared_data[:, :, 2]\n","      # print('prep_out shape', prep_out.shape)\n","      prep_inp = self.prepared_data\n","\n","      x_train = []\n","      y_train = []\n","      x_test = []\n","      y_test = []\n","\n","      # calc the # of light curves for train vs test\n","      num_lcs = len(prep_inp)\n","      train_perc = round(1.0 * num_lcs)\n","      test_perc = round(num_lcs*0.2)\n","\n","      # save random indices for training\n","      while len(self.train_indx) != train_perc:\n","          indx = random.randint(0, num_lcs-1)\n","          self.train_indx.add(indx)\n","\n","      # save random indices for testint -> no duplicates from training\n","      while len(self.test_indx) <= test_perc:\n","          indx = random.randint(0, num_lcs-1)\n","          # if indx not in self.train_indx:\n","          self.test_indx.add(indx)\n","\n","      # extract training data\n","      for ind in self.train_indx:\n","          x_train.append(prep_inp[ind])\n","          y_train.append(prep_out[ind])\n","\n","      # extract testing data\n","      for ind in self.test_indx:\n","          x_test.append(prep_inp[ind])\n","          y_test.append(prep_out[ind])\n","\n","      # change to numpy arrays\n","      x_train = np.array(x_train).astype(np.float64)\n","      x_test = np.array(x_test).astype(np.float64)\n","      y_train = np.array(y_train).astype(np.float64)\n","      y_test = np.array(y_test).astype(np.float64)\n","\n","      print('shape of prep_inp and x_train:', prep_inp.shape, x_train.shape)\n","      print('shape of prep_out and y_train:', prep_out.shape, y_train.shape)\n","\n","      return x_train, x_test, y_train, y_test\n","\n","  def train_model(self, x_train, x_test, y_train, y_test):\n","      \"\"\"\n","      Trains the NN on training data\n","\n","      Returns the trained model.\n","      \"\"\"\n","      # fit model\n","      train_inp_two = x_train[:, :, :2]\n","      assert (train_inp_two.shape == (\n","          x_train.shape[0], x_train.shape[1], 2))\n","\n","      test_inp_two = x_test[:, :, :2]\n","      assert (test_inp_two.shape == (\n","          x_test.shape[0], x_test.shape[1], 2))\n","\n","      print('fitting model...')\n","      history = self.fit([x_train, train_inp_two], y_train, epochs=self.epochs, batch_size=self.batch_size,\n","                          validation_data=([x_test, test_inp_two], y_test), verbose=1, shuffle=False)\n","\n","      plt.plot(history.history['loss'])\n","      plt.plot(history.history['val_loss'])\n","      plt.title('model loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","\n","  def test_model(self, x_test, y_test, amount = None):\n","      \"\"\"\n","      Uses test data to and NN to predict light curve decodings.\n","\n","      Plots reconstructed light curved from the model prediction vs the orignal curve.\n","      \"\"\"\n","      if amount:\n","        indices = random.sample(range(len(x_test)), k=amount)\n","        x_test = np.array([x_test[i] for i in indices])\n","        y_test = np.array([y_test[i] for i in indices])\n","      \n","\n","      test_inp_two = x_test[:, :, :2]\n","\n","      print('test_inp_one shape: ', x_test.shape)\n","      print('test_inp_two shape: ', test_inp_two.shape)\n","\n","      self.summary()\n","\n","      print('predicting...')\n","      for i in tqdm(range(len(x_test))):\n","\n","          # predicted flux\n","          predicted = self.predict([x_test[i].reshape(-1, self.num_timesteps, 4),\n","                                    test_inp_two[i].reshape(-1, self.num_timesteps, 2)])[0]\n","\n","          # if first prediction, print the prediction\n","          if i == 0:\n","              print('shape of predicted data: ', predicted.shape)\n","\n","          self.plot_band_pred(y_test[i], predicted, i, test_inp_two[i])\n","\n","      print(\"done predicting\")\n","\n","  def plot_band_pred(self, raw, pred, num, time_filters):\n","      raw_g_flux    = []\n","      raw_r_flux    = []\n","      raw_tess_flux = []\n","\n","      pred_g_flux    = []\n","      pred_r_flux    = []\n","      pred_tess_flux = []\n","\n","      g_time    = []\n","      r_time    = []\n","      tess_time = []\n","      # print(time_filters)\n","      for i in range(len(time_filters)):\n","        time, filter_ID = time_filters[i]\n","        raw_flux  = raw[i, 0]\n","        pred_flux = pred[i, 0]\n","        if filter_ID == 4.8:\n","          raw_g_flux.append(raw_flux)\n","          pred_g_flux.append(pred_flux)\n","          g_time.append(time)\n","        elif filter_ID == 6.5:\n","          raw_r_flux.append(raw_flux)\n","          pred_r_flux.append(pred_flux)\n","          r_time.append(time)\n","        elif filter_ID == 7.9:\n","          raw_tess_flux.append(raw_flux)\n","          pred_tess_flux.append(pred_flux)\n","          tess_time.append(time)\n","\n","      # plot\n","      # make 1 x 2 figure\n","      fig, (ax1, ax2) = plt.subplots(2, sharey = True)\n","      fig.suptitle('True vs Decoded Light Curves: ' )#+ str(light_curve_names[num]))\n","\n","      # pred_time = range(len(pred_flux))\n","      # raw_time = range(len(raw_flux))\n","\n","      # plot raw data\n","      ax1.scatter(g_time, raw_g_flux, label='g-band', color = 'green')\n","      ax1.scatter(r_time, raw_r_flux, label='r-band', color = 'red')\n","      ax1.scatter(tess_time, raw_tess_flux, label='tess-band')\n","      ax1.set_ylabel('actual')\n","\n","      # plot predicted data\n","      ax2.set_ylabel('predicted')\n","      ax2.scatter(g_time, pred_g_flux, label='g-band', color='green')\n","      ax2.scatter(r_time, pred_r_flux, label='r-band', color='red')\n","      ax2.scatter(tess_time, pred_tess_flux, label='tess-band')\n","      # save image\n","      fig.show()\n","      fig.savefig(path_to_module + \"light_curve_plots/\" + str(num) + \".png\")\n","      files.download(path_to_module + \"light_curve_plots/\" + str(num) + \".png\")\n","      \n","\n","\n","  def t_SNE_plot(self, labels):\n","      \"\"\"\n","      Constructs 2D plots of light curves in latent space.\n","      \"\"\"\n","      print('using t-SNE...')\n","\n","      # extract all training label indexes\n","      indxs = [i for i in range(len(labels))]\n","      label_set = ['II', 'Ibc', 'Kilonova', 'SLSN-I', 'SNIa-91bg', 'SNIa-norm', 'SNIa-x', 'TDE']\n","      label_colors_map = {label:i for i, label in enumerate(label_set)}\n","      colors = [label_colors_map[label] for label in labels]\n","      # labeled_data = np.array([self.prepared_data[i] for i in indxs])\n","\n","      _, _, z = tqdm(self.encoder.predict(self.prepared_data))\n","\n","      t_sne = TSNE(n_components=2, learning_rate='auto',\n","                    init='random').fit_transform(z)\n","      print('t-sne shape: ', t_sne.shape)\n","\n","      plt.figure(figsize=(12, 10))\n","      plt.scatter(t_sne[:, 0], t_sne[:, 1], c=colors)\n","      plt.colorbar()\n","      plt.title(\"t-SNE with only labeled data\")\n","      plt.show()\n","\n","      # # include unlabeled data\n","      # labels = np.array([c.loc[0, 'Class'] for c in light_curves])\n","      # data = self.prepared_data\n","\n","      # _, _, z = tqdm(encoder.predict(data))\n","\n","      # t_sne = TSNE(n_components=2, learning_rate='auto',\n","      #               init='random').fit_transform(z)\n","      # print('t-sne shape: ', t_sne.shape)\n","\n","      # plt.figure(figsize=(12, 10))\n","      # plt.scatter(t_sne[:, 0], t_sne[:, 1], c=labels)\n","      # plt.colorbar()\n","      # plt.title(\"t-SNE with labeled and unlabeled data\")\n","      # #plt.savefig(self.filepath+'plots/unlabeled-t-sne-latent-space.png', facecolor='white')\n","      # plt.show()\n","\n","  "]},{"cell_type":"code","source":["vae = VAE(dataset, dataset.shape)\n","vae.encoder.summary()\n","vae.decoder.summary()"],"metadata":{"id":"hU6S0cimT_W1"},"id":"hU6S0cimT_W1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"cda5d591","metadata":{"id":"cda5d591"},"outputs":[],"source":["# print(vae.prepared_data.shape)\n","x_train, x_test, y_train, y_test = vae.split_prep_data()\n","check_pt_path = path_to_module + \"chck_pts_sim/separated_classes/\"\n","\n","# training loop:\n","# after training model for 20 epochs, save it. Do this 25 iterations to train for 500 epochs\n","optimizer = tf.keras.optimizers.Adam(clipvalue = 1.0)\n","vae.compile(optimizer=optimizer, loss = vae.reconstruction_loss)\n","vae.epochs = 10\n","for check_pt_numb in range(25):\n","  vae.train_model(x_train, x_test, y_train, y_test)\n","  vae.save(check_pt_path + 'ckpt_' + str(check_pt_numb))\n"]},{"cell_type":"code","execution_count":null,"id":"7b579c6e","metadata":{"id":"7b579c6e"},"outputs":[],"source":["# test the trained model\n","x_train, x_test, y_train, y_test = vae.split_prep_data()\n","vae.test_model(x_test,y_test, 40)"]},{"cell_type":"code","execution_count":null,"id":"b9c93390","metadata":{"id":"b9c93390"},"outputs":[],"source":["rvae = VAE(dataset, dataset.shape)\n","vae = keras.models.load_model(path_to_module + \"chck_pts_sim/separated_classes/ckpt_11\", custom_objects={\n","    'VAE':VAE, 'Encoder':Encoder, \n","    'Decoder':Decoder, 'Sampling':Sampling,\n","    'reconstruction_loss':rvae.reconstruction_loss})"]},{"cell_type":"code","source":["vae.summary()\n","vae.encoder.summary()\n","# vae.encoder.input_mask.summary()"],"metadata":{"id":"WqzUSZ39GMNh"},"id":"WqzUSZ39GMNh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoder masks: used to see the masking of the layers within the model\n","print(\"ENCODER MASKS\")\n","print(vae.encoder.layers)\n","vae.encoder.build(dataset.shape)\n","for i, l in enumerate(vae.encoder.layers):\n","    print(f'layer {i}: {l}')\n","    print(f'has input mask: {l.input_mask}')\n","    print(f'has output mask: {l.output_mask}')\n","\n","# Decoder masks: used to see the masking of the layers within the model\n","print(\"DECODER MASKS\")\n","for i, l in enumerate(vae.decoder.layers):\n","    print(f'layer {i}: {l}')\n","    # print(l.input_mask)\n","    print(f'has input mask: {l.input_mask}')\n","    print(f'has output mask: {l.output_mask}')\n","\n","# VAE masks: used to see the masking of the layers within the model\n","print(\"VAE MASKS\")\n","for i, l in enumerate(vae.layers):\n","    print(f'layer {i}: {l}')\n","    print(f'has input mask: {l.input_mask}')\n","    print(f'has output mask: {l.output_mask}')"],"metadata":{"id":"3eZH6_ygHJki"},"id":"3eZH6_ygHJki","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vae.t_SNE_plot = rvae.t_SNE_plot \n","rvae = VAE(dataset, dataset.shape)\n","vae.t_SNE_plot = rvae.t_SNE_plot\n","vae.t_SNE_plot(labels)"],"metadata":{"id":"lH9vd-bZ9-G4"},"id":"lH9vd-bZ9-G4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c3718770","metadata":{"scrolled":true,"id":"c3718770"},"outputs":[],"source":["from imblearn.ensemble import BalancedRandomForestClassifier\n","import pandas as pd\n","import numpy as np\n","import random\n","import joblib\n","from collections import Counter\n","import tensorflow as tf\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier\n","import matplotlib.pyplot as plt\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":null,"id":"d0dd189c","metadata":{"id":"d0dd189c"},"outputs":[],"source":["class RandomForest:\n","\n","    def __init__(self, labels, prepared_data, encoder):\n","\n","        # initialize rvae\n","        self.rvae = VAE(prepared_data, prepared_data.shape)\n","\n","        # encoded dimension from NN\n","        self.encoded_dim = self.rvae.latent_dim\n","\n","        # get training encoder\n","        self.encoder = encoder\n","\n","        # augmented data frame\n","        # self.light_curves = light_curves\n","        self.labels = labels\n","\n","        # prepared data\n","        self.prepared_data = prepared_data\n","\n","    def create_test_train(self):\n","        \"\"\"\n","        Splits data into 85% training, 15% testing, and unlabeled\n","        \"\"\"\n","\n","        print(\"Splitting data for RF...\")\n","\n","        # extract all class outputs and inputs\n","        prep_out = self.labels# np.array([c.loc[0, 'Class'] for c in self.light_curves])\n","        prep_inp = self.prepared_data\n","\n","        # extract all training label indexes\n","        train_indx = [i for i in range(len(prep_out)) if prep_out[i] != 3]\n","        unclassified_indx = [i for i in range(\n","            len(prep_out)) if prep_out[i] == 3]\n","        num_indxs = len(train_indx)\n","\n","        x_train = []\n","        y_train = []\n","        x_test = []\n","        y_test = []\n","        x_unclassified = []\n","\n","        # extract training data\n","        while len(x_train) < int(num_indxs*0.85):\n","            ran = random.randint(0, len(train_indx)-1)\n","            ind = train_indx[ran]\n","            train_indx.remove(ind)\n","            x_train.append(prep_inp[ind])\n","            y_train.append(prep_out[ind])\n","\n","        # append the rest of the data to testing\n","        for ind in train_indx:\n","            x_test.append(prep_inp[ind])\n","            y_test.append(prep_out[ind])\n","\n","        # extract unclassified data\n","        for ind in unclassified_indx:\n","            x_unclassified.append(prep_inp[ind])\n","\n","        # change to numpy arrays\n","        x_train = np.array(x_train)\n","        x_test = np.array(x_test)\n","        y_train = np.array(y_train)\n","        y_test = np.array(y_test)\n","        x_unclassified = np.array(x_unclassified)\n","\n","        # change y arrays to 1-d arrays\n","        y_train.shape = (y_train.shape[0],)\n","        y_test.shape = (y_test.shape[0],)\n","\n","        print('shape of x_train and x_test:', x_train.shape, x_test.shape)\n","        print('shape of y_train and y_test:', y_train.shape, y_test.shape)\n","        print('shape of x_unclassified:', x_unclassified.shape)\n","\n","        return x_train, x_test, y_train, y_test, x_unclassified\n","\n","    def make_encodings(self, x_train, x_test, x_unclassified):\n","        \"\"\"\n","        Uses trained encoder to produce 1D encodings of light curves to be used for RF training\n","        \"\"\"\n","        print('making encodings...')\n","\n","        # encode training light curves\n","        x_train_enc = self.encoder.predict(\n","            x_train, workers=32, use_multiprocessing=True, batch_size=128, verbose=1)[2]\n","        x_test_enc = self.encoder.predict(\n","            x_test, workers=32, use_multiprocessing=True, batch_size=128, verbose=1)[2]\n","        if len(x_unclassified) > 0:\n","          x_unclassified_enc = self.encoder.predict(\n","              x_unclassified, workers=32, use_multiprocessing=True, batch_size=64, verbose=1)[2]\n","        else:\n","          x_unclassified_enc = [[]]\n","        # x_train_enc = self.encoder.predict_on_batch(x_train)\n","        # x_test_enc  = self.encoder.predict_on_batch(x_test)\n","        # x_unclassified_enc = self.encoder.predict_on_batch(x_unclassified)\n","        \n","        # numpy arrays\n","        x_test_enc = np.array(x_test_enc)\n","        x_train_enc = np.array(x_train_enc)\n","        x_unclassified_enc = np.array(x_unclassified_enc)\n","\n","        print('shape of encodings: ', x_train_enc.shape,\n","              x_test_enc.shape, x_unclassified_enc.shape)\n","\n","        return x_train_enc, x_test_enc, x_unclassified_enc\n","\n","    def build_classier(self, x_train, x_test, x_unclassified, y_train, y_test):\n","        \"\"\"\n","        Trains a RF classifier and tests its prediction accuracy\n","        \"\"\"\n","        print('building classifier...')\n","        rf = RandomForestClassifier(n_estimators=2000, class_weight = 'balanced')#, min_samples_split=40 )\n","\n","        # reshape\n","        x_train = x_train.reshape(-1, self.encoded_dim)\n","        x_test = x_test.reshape(-1, self.encoded_dim)\n","        x_unclassified = x_unclassified.reshape(-1, self.encoded_dim)\n","\n","\n","        print('shape of encodings: ', x_train.shape,\n","              x_test.shape, x_unclassified.shape)\n","\n","        # fit to data\n","        rf.fit(x_train, y_train)\n","        \n","        # performing predictions on the test dataset\n","        number_to_class = {0:'SNIa', 1:'SNIbc', 2:'SNIi', 3:'Unclassified', 4:'Other'}\n","        y_pred = rf.predict(x_test)\n","\n","        y_train_counter = {key:value for (key, value) in Counter(y_train).items()}\n","        y_test_counter = {key:value for (key, value) in Counter(y_test).items()}\n","        y_pred_counter = {key:value for (key, value) in Counter(y_pred).items()}\n","        print('y_train counts: ', y_train_counter)\n","        print('y_test counts: ', y_test_counter)\n","        print('y_pred counts: ', y_pred_counter)\n","\n","        # check accuracy\n","        print(\"ACCURACY OF THE MODEl: \", 100 *\n","              round(rf.score(x_test, y_test), 2), '%')\n","\n","       # Create confusion matrix\n","        conf_mat = pd.crosstab(y_test, y_pred, rownames=[\n","                               'Actual Species'], colnames=['Predicted Species'])\n","\n","        print('Confusion Matrix:')\n","        print(conf_mat.to_string())\n","\n","        fig, ax = plt.subplots(figsize=(12, 12))\n","        plot_confusion_matrix(rf, x_test, y_test, display_labels = ['Ibc', 'SNIa-x', 'Kilonova', 'II', 'SNIa-norm', 'SNIa-91bg', 'SLSN-I', 'TDE'], ax=ax)\n","        plt.show()\n","\n","        # print('Unlabeled Classifications: ')\n","        # unlabeled = rf.predict(x_unclassified)\n","        # print(unlabeled)\n","        # print(Counter(unlabeled))\n","\n","        return rf\n","\n","    def classify(self, rf, index, filename = None):\n","        \"\"\"\n","        Classifies a specific light curve.\n","        \"\"\"\n","        print('classifying specific light curve data...')\n","\n","        # load file data if file is passed\n","        if filename:\n","            raw_df = original_curves\n","            names = raw_df.loc[:, 'Filename']\n","\n","            for i in range(len(names)):\n","                if names[i] == filename:\n","                    indx = i\n","\n","            data = self.prepared_data[indx]\n","            correct = raw_df.loc[indx]['Class']\n","\n","        # reshape\n","        # data = data.reshape(1, self.rvae.num_timesteps, self.rvae.num_feats)\n","\n","        # encode data\n","        data = self.encoder.predict(data)[2]\n","\n","        # make class num -> classification dict\n","        classes = {0: 'SNIa', 1: 'SNIbc', 2: 'SNII',\n","                   3: 'Other', 4: 'Unclassified'}\n","\n","        # make prediction from data\n","        pred = rf.predict(data)\n","\n","        # print confidence\n","        probs = np.array(rf.predict_proba(data)[0])\n","        print('Number of different possible predictions: ', len(probs))\n","        highest_prob_ind = np.argmax(probs)\n","        highest_prob = max(probs)\n","        print('Prediction is ' + classes[highest_prob_ind]+' (',\n","              pred[0], ') with '+str(int(highest_prob*100))+'% confidence')\n","        print('Correct classification should be: ',\n","              classes[correct], ' (', correct, ')')\n","\n","# %%"]},{"cell_type":"code","execution_count":null,"id":"e1f60199","metadata":{"id":"e1f60199"},"outputs":[],"source":["# print(light_curves)\n","# rvae = VAE(prepared_data, prepared_data.shape)\n","# vae = keras.models.load_model(path_to_module + \"chck_pts_sim/ckpt_100_unit_data\", custom_objects={'VAE':VAE, 'Encoder':Encoder, \n","#                                                                                                                               'Decoder':Decoder, 'Sampling':Sampling,\n","#                                                                                                                               'reconstruction_loss':rvae.reconstruction_loss})\n","rf= RandomForest(labels,dataset, vae.encoder)\n","\n","\n","# split data set for supervised training\n","x_train, x_test, y_train, y_test, x_unclassified= rf.create_test_train()\n","# print(x_train)\n","# encode input data\n","x_train_enc,x_test_enc,x_unclassified_enc=rf.make_encodings(x_train, x_test, x_unclassified)\n","\n","# build and train the classifier\n","classifier = rf.build_classier(x_train_enc,x_test_enc,x_unclassified_enc,y_train,y_test)\n","\n"]},{"cell_type":"code","source":["indexes=[0, 4000]\n","for i in indexes:\n","  correct = labels[i]\n","  # shape should be (1, 300, 4)\n","  light_curve = np.array([dataset[i]])\n","  encoded_lc = rf.encoder.predict(light_curve)[2]\n","  # make prediction from data\n","  pred = classifier.predict(encoded_lc)\n","  # make class num -> classification dict\n","  classes = {0: 'SNIa', 1: 'SNIbc', 2: 'SNII',\n","              3: 'Other', 4: 'Unclassified'}\n","\n","  # print confidence\n","  probs = np.array(classifier.predict_proba(encoded_lc)[0])\n","  print('Number of different possible predictions: ', len(probs))\n","  highest_prob_ind = np.argmax(probs)\n","  highest_prob = probs[highest_prob_ind]\n","  print(\"Probabilities:\", probs)\n","  print('Prediction is ' + classes[highest_prob_ind]+' (',\n","        pred[0], ') with '+str(int(highest_prob*100))+'% confidence')\n","  print('Correct classification should be: ',\n","        classes[correct], ' (', correct, ')')\n","  print()"],"metadata":{"id":"VZ0FNGQeHRua"},"id":"VZ0FNGQeHRua","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[{"file_id":"1QpFxA9yT5qmMyOSXTkFBZn3OskQRmFNn","timestamp":1675628042574}]},"gpuClass":"premium","accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}